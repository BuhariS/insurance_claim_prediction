{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0ac6b9",
   "metadata": {},
   "source": [
    "# Title\n",
    "\n",
    "<img src=\"image.png\" alt=\"image name\" style=\"max-width: 100%;\">\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">1. Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">2. Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">3. Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#model\">4. Model Selection and Evaluation</a></li>\n",
    "<li><a href=\"#conclusions\">5. Conclusions</a></li>\n",
    "<li><a href=\"#references\">6. References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a438d3",
   "metadata": {},
   "source": [
    "<div id='intro'></div>\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "\n",
    "### 1.1 Dataset description\n",
    "\n",
    "\n",
    "### 1.2 Installing and importing dependencies\n",
    "The following packages are essential to running this project successfully: `numpy, pandas, matplotlib, seaborn, sklearn, sidetable, imbalanced-learn and xgboost`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some installations\n",
    "! pip install -q sidetable\n",
    "\n",
    "# installing kaggle\n",
    "! pip install -q kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9eaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data analysis libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importing visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "# overriding matplotlib \n",
    "sns.set()\n",
    "\n",
    "# import datetime\n",
    "import datetime as dt\n",
    "\n",
    "# import the preprocessing classes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import train/test split module\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import linear regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# import metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# import warnings\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd0132",
   "metadata": {},
   "source": [
    "### 1.3 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606eea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset from Kaggle\n",
    "! kaggle_api -q\n",
    "\n",
    "# unzip the dataset\n",
    "! unzip -q file.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "_df = pd.read_csv('')\n",
    "_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3c102",
   "metadata": {},
   "source": [
    "<div id='wrangling'></div>\n",
    "\n",
    "## 2. Data Wrangling\n",
    "\n",
    "**What is the size, data type and are there missing values in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b88292",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a33b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aliter\n",
    "df.shape\n",
    "\n",
    "# count missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# percentage missing values\n",
    "df.isnull().sum() / df.shape[0] * 100\n",
    "\n",
    "# impute the missing values in the Age column by mean Value\n",
    "df['Age'].fillna(df['Age'].mean(),inplace=True)\n",
    "\n",
    "df['Embarked'].fillna(df['Embarked'].mode().item(),inplace=True)\n",
    "\n",
    "#check\n",
    "\n",
    "#rename cols\n",
    "df=df.rename(columns={'Sex':'Gender','Name':'Full Name'})\n",
    "\n",
    "# adding and modifying cols\n",
    "df['last_name']=df['Full Name'].apply(lambda x: x.split(',')[0])\n",
    "df['first_name']=df['Full Name'].apply(lambda x: ' '.join(x.split(',')[1:]))\n",
    "\n",
    "#adding rows\n",
    "row=dict({'Age':24,'Full Name':'Rohith','Survived':'Y'})\n",
    "df=df.append(row,ignore_index=True)\n",
    "df.tail()\n",
    "\n",
    "#delete rows\n",
    "df=df.drop(df.index[-1],axis=0) # Deletes last row\n",
    "df.head()\n",
    "\n",
    "#encode categorical to numerical for ml\n",
    "df['Gender']=df['Gender'].map({\"male\":'0',\"female\":\"1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select column based on data type\n",
    "df.select_dtypes(include=['int64'], exclude=['str'])\n",
    "\n",
    "# select columns based on list of col names\n",
    "df[['Col1', 'Col2']]\n",
    "\n",
    "# selecting columns where column name contains 'Average' string\n",
    "df.filter(like='Average') # has regex version check it out\n",
    "\n",
    "# .loc single column selection \n",
    "df.loc[:, 'Food' ]\n",
    "\n",
    "# .loc multiple column selection\n",
    "df.loc[:, ['State', 'Food'] ]\n",
    "\n",
    "# .iloc single column selection \n",
    "df.iloc[:, 0]\n",
    "\n",
    "# .iloc multiple column selection \n",
    "df.iloc[:, [0, 2, 3] ]\n",
    "\n",
    "#*************************\n",
    "# Creating the boolean mask\n",
    "booleanMask = df.columns.isin(['State', 'Food'])\n",
    "# saving the selected columns \n",
    "selectedCols = df.columns[booleanMask]\n",
    "# selecting the desired columns\n",
    "df[selectedCols]\n",
    "#**************************\n",
    "\n",
    "# slicing df.columns to access the last two columns of the dataframe\n",
    "\n",
    "df[df.columns[-2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1788e2",
   "metadata": {},
   "source": [
    "Issue #8\n",
    "\n",
    "Define\n",
    "\n",
    "Code\n",
    "\n",
    "Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd82f6",
   "metadata": {},
   "source": [
    "**Are there duplicates in the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576acc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count duplicates\n",
    "_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22556e48",
   "metadata": {},
   "source": [
    "**Are there outliers in the dataset?**\n",
    "\n",
    "Below are some of the methods of treating the outliers\n",
    "\n",
    "Trimming/removing the outlier\n",
    "\n",
    "Quantile based flooring and capping\n",
    "\n",
    "Mean/Median imputation\n",
    "[source](https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outliers(df):\n",
    "  # select numeric columns\n",
    "  df_numeric = df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "  # get column names\n",
    "  columns = df_numeric.columns\n",
    "\n",
    "  # find the name of all columns with outliers\n",
    "  outlier_cols = [col for col in columns if len(find_outlier_rows(df_numeric, col)) != 0]\n",
    "\n",
    "  # dataframe to store the results\n",
    "  outliers_df = pd.DataFrame(columns=['outlier_counts', 'outlier_percent'])\n",
    "\n",
    "  # count the outliers and compute the percentage of outliers for each column\n",
    "  for col in outlier_cols:\n",
    "    outlier_count = len(find_outlier_rows(df_numeric, col))\n",
    "    all_entries = len(df[col])\n",
    "    outlier_percent = round(outlier_count*100/all_entries, 2)\n",
    "\n",
    "    # store the results in the dataframe\n",
    "    outliers_df.loc[col] = [outlier_count, outlier_percent]\n",
    "\n",
    "  # return the resulting dataframe\n",
    "  return outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outlier_rows(df, col, level='both'):\n",
    "  \"\"\"\n",
    "  Finds the rows with outliers in a given column of a dataframe.\n",
    "  \n",
    "  This function takes a dataframe and a column as input, and returns the rows\n",
    "  with outliers in the given column. Outliers are identified using the\n",
    "  interquartile range (IQR) formula. The optional level parameter allows the\n",
    "  caller to specify the level of outliers to return, i.e., lower, upper, or both.\n",
    "  \n",
    "  Args:\n",
    "    df: The input dataframe.\n",
    "    col: The name of the column to search for outliers.\n",
    "    level: The level of outliers to return, i.e., 'lower', 'upper', or 'both'.\n",
    "           Defaults to 'both'.\n",
    "  \n",
    "  Returns:\n",
    "    A dataframe containing the rows with outliers in the given column.\n",
    "  \"\"\"\n",
    "  # compute the interquartile range\n",
    "  iqr = df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "\n",
    "  # compute the upper and lower bounds for identifying outliers\n",
    "  lower_bound = df[col].quantile(0.25) - 1.5 * iqr\n",
    "  upper_bound = df[col].quantile(0.75) + 1.5 * iqr\n",
    "\n",
    "  # filter the rows based on the level of outliers to return\n",
    "  if level == 'lower':\n",
    "    return df[df[col] < lower_bound]\n",
    "  elif level == 'upper':\n",
    "    return df[df[col] > upper_bound]\n",
    "  else:\n",
    "    return df[(df[col] > upper_bound) | (df[col] < lower_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94059d1c",
   "metadata": {},
   "source": [
    "**What are the summary statistics of the dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8950063",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbf010",
   "metadata": {},
   "source": [
    "<div id='eda'></div>\n",
    "\n",
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "**Univariate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994667d",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms\n",
    "\n",
    "df[list_numerical_feat].hist(figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot\n",
    "df[list_numerical_feat].plot(kind='density', subplots=True, layout=(3, 2), sharex=False, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88905e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot\n",
    "sns.boxplot(x='bmi', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plot\n",
    "sns.violinplot(x='bmi', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "sns.lineplot(x=sales.date, y=(sales.weekly_sales/1e6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Sales (in million USD)')\n",
    "plt.title('Weekly Sales Trend',fontdict={'fontsize': 16, 'color':'red'}, pad=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b75f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool tip\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, figsize=(12,6))\n",
    "# plot and labels\n",
    "sc = ax.scatter(x,y)\n",
    "plt.xlabel(x_name)\n",
    "plt.ylabel(y_name)\n",
    "# cursor grid lines\n",
    "lnx = plt.plot([60,60], [0,1.5], color='black', linewidth=0.3)\n",
    "lny = plt.plot([0,100], [1.5,1.5], color='black', linewidth=0.3)\n",
    "lnx[0].set_linestyle('None')\n",
    "lny[0].set_linestyle('None')\n",
    "# annotation\n",
    "annot = ax.annotate(\"\", xy=(0,0), xytext=(5,5),textcoords=\"offset points\")\n",
    "annot.set_visible(False)\n",
    "# xy limits\n",
    "plt.xlim(x.min()*0.95, x.max()*1.05)\n",
    "plt.ylim(y.min()*0.95, y.max()*1.05)\n",
    "def hover(event):\n",
    "    # check if event was in the axis\n",
    "    if event.inaxes == ax:\n",
    "        # draw lines and make sure they're visible\n",
    "        lnx[0].set_data([event.xdata, event.xdata], [0, 1.5])\n",
    "        lnx[0].set_linestyle('--')\n",
    "        lny[0].set_data([0,100], [event.ydata, event.ydata])\n",
    "        lny[0].set_linestyle('--')\n",
    "        lnx[0].set_visible(True)\n",
    "        lny[0].set_visible(True)\n",
    "        \n",
    "        # get the points contained in the event\n",
    "        cont, ind = sc.contains(event)\n",
    "        if cont:\n",
    "            # change annotation position\n",
    "            annot.xy = (event.xdata, event.ydata)\n",
    "            # write the name of every point contained in the event\n",
    "            annot.set_text(\"{}\".format(', '.join([tt[n] for n in ind[\"ind\"]])))\n",
    "            annot.set_visible(True)    \n",
    "        else:\n",
    "            annot.set_visible(False)\n",
    "    else:\n",
    "        lnx[0].set_visible(False)\n",
    "        lny[0].set_visible(False)\n",
    "fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284d506",
   "metadata": {},
   "source": [
    "**Multivariate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad71e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603081ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustered bar chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "sns.scatterplot(x='age',y='bmi',hue='insurance_claim', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot\n",
    "\n",
    "sns.set_style(\"whitegrid\");\n",
    "sns.pairplot(df, hue=\"insurance_claim\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bubble plot\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(x=\"bmi\",\n",
    "                y=\"claim_amount\",\n",
    "                size=\"children\",\n",
    "                sizes=(20,100),\n",
    "                alpha=0.8,\n",
    "                hue=\"smoker\",\n",
    "                data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint plot\n",
    "sns.jointplot(x = 'age', y = 'bmi', data = df);\n",
    " \n",
    "sns.jointplot(x = 'age', y = 'bmi', data = df, hue='insurance_claim');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8600687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot\n",
    "plt.plot(np.arange(len(df.Y)), df.Y)\n",
    "plt.title(\"ZAR/USD over time\")\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"ZAR/USD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eae734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 in 1 line plot\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n",
    "# Create blank figure\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "# Split figure to allow two sets of y axes\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "# Plot the first line on its axis\n",
    "ax.plot(np.arange(len(df.Y)), df.Y, '-', label = 'ZAR/USD', color='orange')\n",
    "\n",
    "\n",
    "# Create second y axis and plot second line\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(np.arange(len(df.X)), df.X, '-', label = 'Exports (ZAR)')\n",
    "\n",
    "\n",
    "# Add legends for each axis\n",
    "ax.legend(loc=2)\n",
    "ax2.legend(loc=9)\n",
    "\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "\n",
    "# Set labels of axes\n",
    "ax.set_xlabel(\"Months\")\n",
    "ax.set_ylabel(\"ZAR/USD\")\n",
    "ax2.set_ylabel(\"Exports (ZAR, millions)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638598d",
   "metadata": {},
   "source": [
    "**Issues**\n",
    "\n",
    "***Predictors issues***\n",
    "- `linearity`: when we see that the Xs are not linear from the scatter plots of Xs and y we may need to transform the Xs and/or y with functions like log(x) and check to see if there is linearity then apply a linear function.\n",
    "- `collinearity`/- `multicollinearity`: when Xs are correlated with one another. This makes it hard to determine the effect of Xs on y. Similar to collinearity. This can be checked with varience inflation factor (VIF). **0-5 good, 6-10: pay attention can remove and leave one of those, >10 remove**\n",
    "\n",
    "If we fit a regression model to a dataset that is non-linear, it will fail to adequately capture the relationship in the data - resulting in a mathematically inappropriate model. In order to check for linearity, we can produce scatter plots of each individual predictor against the response variable. The intuition here is that we are looking for obvious linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(14,6),)\n",
    "fig.subplots_adjust(hspace = 0.5, wspace=.2)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for index, column in enumerate(df.columns):\n",
    "    axs[index-1].set_title(\"{} vs. mpg\".format(column),fontsize=16)\n",
    "    axs[index-1].scatter(x=df[column],y=df['mpg'],color='blue',edgecolor='k')\n",
    "\n",
    "fig.tight_layout(pad=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04daf078",
   "metadata": {},
   "source": [
    "Checking for Multicollinearity\n",
    "Multicollinearity refers to the presence of strong correlation among two or more of the predictor variables in the dataset. The presence of any correlation among predictors is detrimental to model quality for two reasons:\n",
    "\n",
    "It tends to increase the standard error;\n",
    "\n",
    "It becomes difficult to estimate the effect of any one predictor variable on the response variable.\n",
    "\n",
    "We will check for multicollinearity by generating pairwise scatter plots among predictors, and further, generating a correlation heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf58bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the number of visuals created, this codeblock takes about one minute to run.\n",
    "from seaborn import pairplot\n",
    "g = pairplot(df1.drop('mpg', axis='columns'))\n",
    "g.fig.set_size_inches(9,9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818c093",
   "metadata": {},
   "source": [
    "***residuals issues***\n",
    "- `independence/ autocorrelation` when there is correlation between **residuals**. This is not desirable. It is abd especially for time series models. This can be checked by plotting the residuals and check for rectangular patterns. Can also be checked using Durbin-Watson stat: desired value approx 2, 0-2 positive autocorrelation, 2-4 negative autocorrelation\n",
    "- `homoscadacity`: constant varience among residuals for all fitted values. this makes it difficult to guage the standard deviation or error of the forecast prediction., narrow confidence interval. This can be checked by plotting residuals against predicted y. We seek a recatangular pattern not funnel one.\n",
    "- `normality`: errors/residuals are generated from normal distribution: may not be necessary, check with quantile-quantile plot, shapiro will"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246090c1",
   "metadata": {},
   "source": [
    "<div id='model'></div>\n",
    "\n",
    "## 4. Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7232da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictor and the predicted variables\n",
    "X = _df.drop(['col'], axis=1)\n",
    "y = _df['col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c8001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiatiating the model\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "#without splitting\n",
    "X = df.X[:,np.newaxis] # convert shape of X from (n,) to (n,1)\n",
    "lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e03c63",
   "metadata": {},
   "source": [
    "**Evaluating the model on train set: how well the line fits the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f08938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "# generate values of y from x, using the linear model\n",
    "y_pred = lm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4171b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', metrics.mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf53fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple MSE by N to get RSS\n",
    "print(\"Residual sum of squares:\", metrics.mean_squared_error(y_train, y_pred)*len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9621ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R_squared:', metrics.r2_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "# Calculate the mean-squared-error\n",
    "print('MSE:', metrics.mean_squared_error(y_train, y_pred))\n",
    "# Calculate the R-squared metric\n",
    "print('R_squared:', metrics.r2_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for slr\n",
    "# plot the results\n",
    "plt.scatter(X, y)  # plot the original data\n",
    "plt.plot(X, gen_y, color='red')  # plot the line connecting the generated y-values\n",
    "plt.ylabel(\"ZAR/USD\")\n",
    "plt.xlabel(\"Value of Exports (ZAR, millions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16cf004",
   "metadata": {},
   "source": [
    "**Testing the model on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69862481",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing:\")\n",
    "print('MSE:', metrics.mean_squared_error(y_test, gen_y_test))\n",
    "print('R_squared:', metrics.r2_score(y_test, gen_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9becfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating MLR\n",
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "# dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_train, slr.predict(X_train[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_train, lm.predict(X_train))\n",
    "                    },\n",
    "                'Test MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_test, lm.predict(X_test))\n",
    "                    },\n",
    "                'Test RMSE':\n",
    "                    {\n",
    "                        \"SLR\": math.sqrt(metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']]))),\n",
    "                        \"MLR\": math.sqrt(metrics.mean_squared_error(y_test, lm.predict(X_test)))\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c7bb1",
   "metadata": {},
   "source": [
    "**Note**\n",
    "If Mean squared error is higher on the test set than the train set, indicating poor predictive accuracy, and R-squared is lower on the test set, indicating a worse fit on the test set.\n",
    "\n",
    "These results indicate a concept in machine learning model fitting known as overfitting. This is a phenomenon where there is:\n",
    "\n",
    "A discrepancy between the performance of the model on train and on test sets; and\n",
    "An inability of the model to generalise to data it has not seen before.\n",
    "The term comes from the fact that the model fits too well, or overfits, the training data, and does not fit well, or underfits, the testing data.\n",
    "\n",
    "In the trains to follow, we'll look at ways to improve model performance as well as prevent or mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb456ce8",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "For big dataset we can have 3 splits: train, validation and test splits. The validation is used to check improvement in hyperparameter tuning. After we are satisfied with that we then use the test set.\n",
    "\n",
    "For small dataset where we can't afford 3 splits we can use cross-validation like k-fold. This can be used to prevent overfitting.\n",
    "\n",
    "**Stat model**\n",
    "\n",
    "SK Learn is limited in terms of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4adf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf44fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating string for regression\n",
    "\n",
    "formula_str = df1.columns[0]+' ~ '+'+'.join(df1.columns[1:]); formula_str\n",
    "\n",
    "# the model \n",
    "model=sm.ols(formula=formula_str, data=df1)\n",
    "fitted = model.fit()\n",
    "# print model stats\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e14b4a",
   "metadata": {},
   "source": [
    "<div id='conclusions'></div>\n",
    "\n",
    "## 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592eb6c",
   "metadata": {},
   "source": [
    "<div id='references'></div>\n",
    "\n",
    "## 6. References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
